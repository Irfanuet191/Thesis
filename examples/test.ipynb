{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "29838002",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'util'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Open and load the pickle file\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m----> 8\u001b[0m     scene_graphs \u001b[38;5;241m=\u001b[39m \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Inspect the structure\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(scene_graphs))\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'util'"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from roadscene2vec import *\n",
    "# Path to your .pkl file\n",
    "file_path = '/home/irfan/Downloads/use_case_data-20250504T102722Z-001/use_case_data/271_carla_scenegraphs.pkl'\n",
    "\n",
    "# Open and load the pickle file\n",
    "with open(file_path, 'rb') as f:\n",
    "    scene_graphs = pickle.load(f)\n",
    "\n",
    "# Inspect the structure\n",
    "print(type(scene_graphs))\n",
    "print(scene_graphs.keys() if isinstance(scene_graphs, dict) else scene_graphs[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea2e667a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/irfan/roadscene2vec/examples'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "722b9dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, pdb\n",
    "from pathlib import Path\n",
    "sys.path.append(str(Path(\"../../\")))\n",
    "%pwd\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ab6cbe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/irfan/roadscene2vec\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/irfan/roadscene2vec'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%cd ../\n",
    "%pwd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca03d393",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.dirname(sys.path[0]))\n",
    "import roadscene2vec\n",
    "from roadscene2vec.util.config_parser import configuration\n",
    "import roadscene2vec.scene_graph.extraction.image_extractor as RealEx\n",
    "from roadscene2vec.learning.util.scenegraph_trainer import Scenegraph_Trainer\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6db74dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]/home/irfan/miniconda3/envs/test_env/lib/python3.9/site-packages/torch/functional.py:539: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:3637.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boxes: Boxes(tensor([[ 785.5082,  385.8770, 1084.0192,  510.8224],\n",
      "        [ 270.3557,  384.6895,  502.0559,  496.8985]], device='cuda:0')), Labels: tensor([2, 2], device='cuda:0'), Image Size: (720, 1280)\n",
      "Detections: [([785.5081787109375, 385.8769836425781, 298.510986328125, 124.94537353515625], 0.5, 2), ([270.35565185546875, 384.6895446777344, 231.70022583007812, 112.208984375], 0.5, 2)]\n",
      "tracks   [<deep_sort_realtime.deep_sort.track.Track object at 0x7aaf582447c0>, <deep_sort_realtime.deep_sort.track.Track object at 0x7aaf582445b0>]\n",
      "(Boxes(tensor([[ 785.5082,  385.8770, 1084.0192,  510.8224],\n",
      "        [ 270.3557,  384.6895,  502.0559,  496.8985]], device='cuda:0')), tensor([2, 2], device='cuda:0'), (720, 1280))\n",
      "Extracting scene graph for sequence 22 frame 97095 with 0 tracks\n",
      "Adding node:  Root Road Root Road\n",
      "Adding node:  ego car ego car\n",
      "Adding node:  Left Lane Left Lane\n",
      "Adding node:  Right Lane Right Lane\n",
      "Adding node:  Middle Lane Middle Lane\n",
      "Boxes:  Boxes(tensor([[ 785.5082,  385.8770, 1084.0192,  510.8224],\n",
      "        [ 270.3557,  384.6895,  502.0559,  496.8985]], device='cuda:0'))\n",
      "Processing bounding box:  car\n",
      "Actor:  ego_car\n",
      "Actor:  car\n",
      "Actor type:  car\n",
      "Actor actor_:  1\n",
      "Actor:  moto\n",
      "Actor:  bicycle\n",
      "Actor:  ped\n",
      "Actor:  lane\n",
      "Actor:  light\n",
      "Actor:  sign\n",
      "Actor:  road\n",
      "Actor type 1:  car\n",
      "Actor type 2:  car\n",
      "list :  []  list  0\n",
      "Adding node:  car_0 car_0\n",
      "Processing bounding box:  car\n",
      "Actor:  ego_car\n",
      "Actor:  car\n",
      "Actor type:  car\n",
      "Actor actor_:  1\n",
      "Actor:  moto\n",
      "Actor:  bicycle\n",
      "Actor:  ped\n",
      "Actor:  lane\n",
      "Actor:  light\n",
      "Actor:  sign\n",
      "Actor:  road\n",
      "Actor type 1:  car\n",
      "Actor type 2:  car\n",
      "list :  []  list  1\n",
      "Adding node:  car_1 car_1\n",
      "Boxes End: \n",
      "Boxes: Boxes(tensor([[ 256.9138,  384.7789,  493.9512,  503.8412],\n",
      "        [ 786.4658,  384.4911, 1093.7416,  520.5203]], device='cuda:0')), Labels: tensor([2, 2], device='cuda:0'), Image Size: (720, 1280)\n",
      "Detections: [([256.9138488769531, 384.7789001464844, 237.037353515625, 119.06234741210938], 0.5, 2), ([786.4657592773438, 384.4910583496094, 307.27581787109375, 136.02926635742188], 0.5, 2)]\n",
      "tracks   [<deep_sort_realtime.deep_sort.track.Track object at 0x7aaf582447c0>, <deep_sort_realtime.deep_sort.track.Track object at 0x7aaf582445b0>]\n",
      "(Boxes(tensor([[ 256.9138,  384.7789,  493.9512,  503.8412],\n",
      "        [ 786.4658,  384.4911, 1093.7416,  520.5203]], device='cuda:0')), tensor([2, 2], device='cuda:0'), (720, 1280))\n",
      "Extracting scene graph for sequence 22 frame 97096 with 0 tracks\n",
      "Adding node:  Root Road Root Road\n",
      "Adding node:  ego car ego car\n",
      "Adding node:  Left Lane Left Lane\n",
      "Adding node:  Right Lane Right Lane\n",
      "Adding node:  Middle Lane Middle Lane\n",
      "Boxes:  Boxes(tensor([[ 256.9138,  384.7789,  493.9512,  503.8412],\n",
      "        [ 786.4658,  384.4911, 1093.7416,  520.5203]], device='cuda:0'))\n",
      "Processing bounding box:  car\n",
      "Actor:  ego_car\n",
      "Actor:  car\n",
      "Actor type:  car\n",
      "Actor actor_:  1\n",
      "Actor:  moto\n",
      "Actor:  bicycle\n",
      "Actor:  ped\n",
      "Actor:  lane\n",
      "Actor:  light\n",
      "Actor:  sign\n",
      "Actor:  road\n",
      "Actor type 1:  car\n",
      "Actor type 2:  car\n",
      "list :  []  list  0\n",
      "Adding node:  car_0 car_0\n",
      "Processing bounding box:  car\n",
      "Actor:  ego_car\n",
      "Actor:  car\n",
      "Actor type:  car\n",
      "Actor actor_:  1\n",
      "Actor:  moto\n",
      "Actor:  bicycle\n",
      "Actor:  ped\n",
      "Actor:  lane\n",
      "Actor:  light\n",
      "Actor:  sign\n",
      "Actor:  road\n",
      "Actor type 1:  car\n",
      "Actor type 2:  car\n",
      "list :  []  list  1\n",
      "Adding node:  car_1 car_1\n",
      "Boxes End: \n",
      "Boxes: Boxes(tensor([[ 231.7877,  387.8305,  481.8097,  512.9922],\n",
      "        [ 787.1014,  383.0298, 1110.3522,  528.0663]], device='cuda:0')), Labels: tensor([2, 2], device='cuda:0'), Image Size: (720, 1280)\n",
      "Detections: [([231.78765869140625, 387.83050537109375, 250.02200317382812, 125.16168212890625], 0.5, 2), ([787.1013793945312, 383.02978515625, 323.25079345703125, 145.03656005859375], 0.5, 2)]\n",
      "tracks   [<deep_sort_realtime.deep_sort.track.Track object at 0x7aaf582447c0>, <deep_sort_realtime.deep_sort.track.Track object at 0x7aaf582445b0>]\n",
      "(Boxes(tensor([[ 231.7877,  387.8305,  481.8097,  512.9922],\n",
      "        [ 787.1014,  383.0298, 1110.3522,  528.0663]], device='cuda:0')), tensor([2, 2], device='cuda:0'), (720, 1280))\n",
      "Extracting scene graph for sequence 22 frame 97097 with 2 tracks\n",
      "Adding node:  Root Road Root Road\n",
      "Adding node:  ego car ego car\n",
      "Adding node:  Left Lane Left Lane\n",
      "Adding node:  Right Lane Right Lane\n",
      "Adding node:  Middle Lane Middle Lane\n",
      "Boxes:  Boxes(tensor([[ 231.7877,  387.8305,  481.8097,  512.9922],\n",
      "        [ 787.1014,  383.0298, 1110.3522,  528.0663]], device='cuda:0'))\n",
      "Processing bounding box:  car\n",
      "Actor:  ego_car\n",
      "Actor:  car\n",
      "Actor type:  car\n",
      "Actor actor_:  1\n",
      "Actor:  moto\n",
      "Actor:  bicycle\n",
      "Actor:  ped\n",
      "Actor:  lane\n",
      "Actor:  light\n",
      "Actor:  sign\n",
      "Actor:  road\n",
      "Actor type 1:  car\n",
      "Actor type 2:  car\n",
      "list :  ['1', '2']  list  0\n",
      "Adding node:  car_0 car_0\n",
      "Processing bounding box:  car\n",
      "Actor:  ego_car\n",
      "Actor:  car\n",
      "Actor type:  car\n",
      "Actor actor_:  1\n",
      "Actor:  moto\n",
      "Actor:  bicycle\n",
      "Actor:  ped\n",
      "Actor:  lane\n",
      "Actor:  light\n",
      "Actor:  sign\n",
      "Actor:  road\n",
      "Actor type 1:  car\n",
      "Actor type 2:  car\n",
      "list :  ['1', '2']  list  1\n",
      "Adding node:  car_1 car_1\n",
      "Boxes End: \n",
      "Boxes: Boxes(tensor([[ 208.3215,  388.2748,  472.3911,  517.9554],\n",
      "        [ 785.8036,  381.2827, 1136.8987,  539.0475]], device='cuda:0')), Labels: tensor([2, 2], device='cuda:0'), Image Size: (720, 1280)\n",
      "Detections: [([208.321533203125, 388.2747802734375, 264.06951904296875, 129.68060302734375], 0.5, 2), ([785.8036499023438, 381.28271484375, 351.09503173828125, 157.76483154296875], 0.5, 2)]\n",
      "tracks   [<deep_sort_realtime.deep_sort.track.Track object at 0x7aaf582447c0>, <deep_sort_realtime.deep_sort.track.Track object at 0x7aaf582445b0>]\n",
      "(Boxes(tensor([[ 208.3215,  388.2748,  472.3911,  517.9554],\n",
      "        [ 785.8036,  381.2827, 1136.8987,  539.0475]], device='cuda:0')), tensor([2, 2], device='cuda:0'), (720, 1280))\n",
      "Extracting scene graph for sequence 22 frame 97098 with 2 tracks\n",
      "Adding node:  Root Road Root Road\n",
      "Adding node:  ego car ego car\n",
      "Adding node:  Left Lane Left Lane\n",
      "Adding node:  Right Lane Right Lane\n",
      "Adding node:  Middle Lane Middle Lane\n",
      "Boxes:  Boxes(tensor([[ 208.3215,  388.2748,  472.3911,  517.9554],\n",
      "        [ 785.8036,  381.2827, 1136.8987,  539.0475]], device='cuda:0'))\n",
      "Processing bounding box:  car\n",
      "Actor:  ego_car\n",
      "Actor:  car\n",
      "Actor type:  car\n",
      "Actor actor_:  1\n",
      "Actor:  moto\n",
      "Actor:  bicycle\n",
      "Actor:  ped\n",
      "Actor:  lane\n",
      "Actor:  light\n",
      "Actor:  sign\n",
      "Actor:  road\n",
      "Actor type 1:  car\n",
      "Actor type 2:  car\n",
      "list :  ['1', '2']  list  0\n",
      "Adding node:  car_0 car_0\n",
      "Processing bounding box:  car\n",
      "Actor:  ego_car\n",
      "Actor:  car\n",
      "Actor type:  car\n",
      "Actor actor_:  1\n",
      "Actor:  moto\n",
      "Actor:  bicycle\n",
      "Actor:  ped\n",
      "Actor:  lane\n",
      "Actor:  light\n",
      "Actor:  sign\n",
      "Actor:  road\n",
      "Actor type 1:  car\n",
      "Actor type 2:  car\n",
      "list :  ['1', '2']  list  1\n",
      "Adding node:  car_1 car_1\n",
      "Boxes End: \n",
      "Boxes: Boxes(tensor([[ 797.3802,  390.9438, 1175.1971,  546.9065],\n",
      "        [ 176.6235,  390.3358,  465.2070,  527.4063]], device='cuda:0')), Labels: tensor([2, 2], device='cuda:0'), Image Size: (720, 1280)\n",
      "Detections: [([797.3802490234375, 390.9437561035156, 377.81689453125, 155.96273803710938], 0.5, 2), ([176.62350463867188, 390.3358459472656, 288.58349609375, 137.07046508789062], 0.5, 2)]\n",
      "tracks   [<deep_sort_realtime.deep_sort.track.Track object at 0x7aaf582447c0>, <deep_sort_realtime.deep_sort.track.Track object at 0x7aaf582445b0>]\n",
      "(Boxes(tensor([[ 797.3802,  390.9438, 1175.1971,  546.9065],\n",
      "        [ 176.6235,  390.3358,  465.2070,  527.4063]], device='cuda:0')), tensor([2, 2], device='cuda:0'), (720, 1280))\n",
      "Extracting scene graph for sequence 22 frame 97099 with 2 tracks\n",
      "Adding node:  Root Road Root Road\n",
      "Adding node:  ego car ego car\n",
      "Adding node:  Left Lane Left Lane\n",
      "Adding node:  Right Lane Right Lane\n",
      "Adding node:  Middle Lane Middle Lane\n",
      "Boxes:  Boxes(tensor([[ 797.3802,  390.9438, 1175.1971,  546.9065],\n",
      "        [ 176.6235,  390.3358,  465.2070,  527.4063]], device='cuda:0'))\n",
      "Processing bounding box:  car\n",
      "Actor:  ego_car\n",
      "Actor:  car\n",
      "Actor type:  car\n",
      "Actor actor_:  1\n",
      "Actor:  moto\n",
      "Actor:  bicycle\n",
      "Actor:  ped\n",
      "Actor:  lane\n",
      "Actor:  light\n",
      "Actor:  sign\n",
      "Actor:  road\n",
      "Actor type 1:  car\n",
      "Actor type 2:  car\n",
      "list :  ['1', '2']  list  0\n",
      "Adding node:  car_0 car_0\n",
      "Processing bounding box:  car\n",
      "Actor:  ego_car\n",
      "Actor:  car\n",
      "Actor type:  car\n",
      "Actor actor_:  1\n",
      "Actor:  moto\n",
      "Actor:  bicycle\n",
      "Actor:  ped\n",
      "Actor:  lane\n",
      "Actor:  light\n",
      "Actor:  sign\n",
      "Actor:  road\n",
      "Actor type 1:  car\n",
      "Actor type 2:  car\n",
      "list :  ['1', '2']  list  1\n",
      "Adding node:  car_1 car_1\n",
      "Boxes End: \n",
      "Boxes: Boxes(tensor([[ 133.6540,  391.6541,  449.0831,  538.9562],\n",
      "        [ 785.0152,  388.5358, 1186.2288,  552.6490],\n",
      "        [ 668.5045,  318.3039,  674.5773,  326.7105]], device='cuda:0')), Labels: tensor([2, 2, 9], device='cuda:0'), Image Size: (720, 1280)\n",
      "Detections: [([133.65402221679688, 391.6540832519531, 315.4291076660156, 147.30215454101562], 0.5, 2), ([785.0151977539062, 388.5357666015625, 401.21356201171875, 164.11322021484375], 0.5, 2), ([668.5044555664062, 318.3038635253906, 6.0728759765625, 8.4066162109375], 0.5, 9)]\n",
      "tracks   [<deep_sort_realtime.deep_sort.track.Track object at 0x7aaf582447c0>, <deep_sort_realtime.deep_sort.track.Track object at 0x7aaf582445b0>, <deep_sort_realtime.deep_sort.track.Track object at 0x7aaf58279880>]\n",
      "(Boxes(tensor([[ 133.6540,  391.6541,  449.0831,  538.9562],\n",
      "        [ 785.0152,  388.5358, 1186.2288,  552.6490],\n",
      "        [ 668.5045,  318.3039,  674.5773,  326.7105]], device='cuda:0')), tensor([2, 2, 9], device='cuda:0'), (720, 1280))\n",
      "Extracting scene graph for sequence 22 frame 97100 with 2 tracks\n",
      "Adding node:  Root Road Root Road\n",
      "Adding node:  ego car ego car\n",
      "Adding node:  Left Lane Left Lane\n",
      "Adding node:  Right Lane Right Lane\n",
      "Adding node:  Middle Lane Middle Lane\n",
      "Boxes:  Boxes(tensor([[ 133.6540,  391.6541,  449.0831,  538.9562],\n",
      "        [ 785.0152,  388.5358, 1186.2288,  552.6490],\n",
      "        [ 668.5045,  318.3039,  674.5773,  326.7105]], device='cuda:0'))\n",
      "Processing bounding box:  car\n",
      "Actor:  ego_car\n",
      "Actor:  car\n",
      "Actor type:  car\n",
      "Actor actor_:  1\n",
      "Actor:  moto\n",
      "Actor:  bicycle\n",
      "Actor:  ped\n",
      "Actor:  lane\n",
      "Actor:  light\n",
      "Actor:  sign\n",
      "Actor:  road\n",
      "Actor type 1:  car\n",
      "Actor type 2:  car\n",
      "list :  ['1', '2']  list  0\n",
      "Adding node:  car_0 car_0\n",
      "Processing bounding box:  car\n",
      "Actor:  ego_car\n",
      "Actor:  car\n",
      "Actor type:  car\n",
      "Actor actor_:  1\n",
      "Actor:  moto\n",
      "Actor:  bicycle\n",
      "Actor:  ped\n",
      "Actor:  lane\n",
      "Actor:  light\n",
      "Actor:  sign\n",
      "Actor:  road\n",
      "Actor type 1:  car\n",
      "Actor type 2:  car\n",
      "list :  ['1', '2']  list  1\n",
      "Adding node:  car_1 car_1\n",
      "Processing bounding box:  traffic light\n",
      "Actor:  ego_car\n",
      "Actor:  car\n",
      "Actor:  moto\n",
      "Actor:  bicycle\n",
      "Actor:  ped\n",
      "Actor:  lane\n",
      "Actor:  light\n",
      "Actor:  sign\n",
      "Actor:  road\n",
      "Actor type 1:  \n",
      "Boxes End: \n",
      "Boxes: Boxes(tensor([[  67.0401,  391.4606,  422.6791,  555.7941],\n",
      "        [ 772.7825,  395.8557, 1193.0306,  567.6339]], device='cuda:0')), Labels: tensor([2, 2], device='cuda:0'), Image Size: (720, 1280)\n",
      "Detections: [([67.04013061523438, 391.4606018066406, 355.63897705078125, 164.33346557617188], 0.5, 2), ([772.782470703125, 395.8557434082031, 420.2481689453125, 171.77810668945312], 0.5, 2)]\n",
      "tracks   [<deep_sort_realtime.deep_sort.track.Track object at 0x7aaf582447c0>, <deep_sort_realtime.deep_sort.track.Track object at 0x7aaf582445b0>]\n",
      "(Boxes(tensor([[  67.0401,  391.4606,  422.6791,  555.7941],\n",
      "        [ 772.7825,  395.8557, 1193.0306,  567.6339]], device='cuda:0')), tensor([2, 2], device='cuda:0'), (720, 1280))\n",
      "Extracting scene graph for sequence 22 frame 97101 with 2 tracks\n",
      "Adding node:  Root Road Root Road\n",
      "Adding node:  ego car ego car\n",
      "Adding node:  Left Lane Left Lane\n",
      "Adding node:  Right Lane Right Lane\n",
      "Adding node:  Middle Lane Middle Lane\n",
      "Boxes:  Boxes(tensor([[  67.0401,  391.4606,  422.6791,  555.7941],\n",
      "        [ 772.7825,  395.8557, 1193.0306,  567.6339]], device='cuda:0'))\n",
      "Processing bounding box:  car\n",
      "Actor:  ego_car\n",
      "Actor:  car\n",
      "Actor type:  car\n",
      "Actor actor_:  1\n",
      "Actor:  moto\n",
      "Actor:  bicycle\n",
      "Actor:  ped\n",
      "Actor:  lane\n",
      "Actor:  light\n",
      "Actor:  sign\n",
      "Actor:  road\n",
      "Actor type 1:  car\n",
      "Actor type 2:  car\n",
      "list :  ['1', '2']  list  0\n",
      "Adding node:  car_0 car_0\n",
      "Processing bounding box:  car\n",
      "Actor:  ego_car\n",
      "Actor:  car\n",
      "Actor type:  car\n",
      "Actor actor_:  1\n",
      "Actor:  moto\n",
      "Actor:  bicycle\n",
      "Actor:  ped\n",
      "Actor:  lane\n",
      "Actor:  light\n",
      "Actor:  sign\n",
      "Actor:  road\n",
      "Actor type 1:  car\n",
      "Actor type 2:  car\n",
      "list :  ['1', '2']  list  1\n",
      "Adding node:  car_1 car_1\n",
      "Boxes End: \n",
      "Boxes: Boxes(tensor([[ 757.8163,  392.7986, 1213.7950,  581.1572],\n",
      "        [   0.0000,  394.0162,  393.3771,  573.7759]], device='cuda:0')), Labels: tensor([2, 2], device='cuda:0'), Image Size: (720, 1280)\n",
      "Detections: [([757.8162841796875, 392.7986145019531, 455.978759765625, 188.35861206054688], 0.5, 2), ([0.0, 394.0162353515625, 393.37713623046875, 179.75970458984375], 0.5, 2)]\n",
      "tracks   [<deep_sort_realtime.deep_sort.track.Track object at 0x7aaf582447c0>, <deep_sort_realtime.deep_sort.track.Track object at 0x7aaf582445b0>]\n",
      "(Boxes(tensor([[ 757.8163,  392.7986, 1213.7950,  581.1572],\n",
      "        [   0.0000,  394.0162,  393.3771,  573.7759]], device='cuda:0')), tensor([2, 2], device='cuda:0'), (720, 1280))\n",
      "Extracting scene graph for sequence 22 frame 97102 with 2 tracks\n",
      "Adding node:  Root Road Root Road\n",
      "Adding node:  ego car ego car\n",
      "Adding node:  Left Lane Left Lane\n",
      "Adding node:  Right Lane Right Lane\n",
      "Adding node:  Middle Lane Middle Lane\n",
      "Boxes:  Boxes(tensor([[ 757.8163,  392.7986, 1213.7950,  581.1572],\n",
      "        [   0.0000,  394.0162,  393.3771,  573.7759]], device='cuda:0'))\n",
      "Processing bounding box:  car\n",
      "Actor:  ego_car\n",
      "Actor:  car\n",
      "Actor type:  car\n",
      "Actor actor_:  1\n",
      "Actor:  moto\n",
      "Actor:  bicycle\n",
      "Actor:  ped\n",
      "Actor:  lane\n",
      "Actor:  light\n",
      "Actor:  sign\n",
      "Actor:  road\n",
      "Actor type 1:  car\n",
      "Actor type 2:  car\n",
      "list :  ['1', '2']  list  0\n",
      "Adding node:  car_0 car_0\n",
      "Processing bounding box:  car\n",
      "Actor:  ego_car\n",
      "Actor:  car\n",
      "Actor type:  car\n",
      "Actor actor_:  1\n",
      "Actor:  moto\n",
      "Actor:  bicycle\n",
      "Actor:  ped\n",
      "Actor:  lane\n",
      "Actor:  light\n",
      "Actor:  sign\n",
      "Actor:  road\n",
      "Actor type 1:  car\n",
      "Actor type 2:  car\n",
      "list :  ['1', '2']  list  1\n",
      "Adding node:  car_1 car_1\n",
      "Boxes End: \n",
      "Boxes: Boxes(tensor([[2.1180e-01, 3.9659e+02, 3.5840e+02, 5.9038e+02],\n",
      "        [7.5225e+02, 3.9151e+02, 1.2173e+03, 5.9553e+02],\n",
      "        [6.3141e+02, 3.1833e+02, 6.3814e+02, 3.2509e+02]], device='cuda:0')), Labels: tensor([2, 2, 9], device='cuda:0'), Image Size: (720, 1280)\n",
      "Detections: [([0.2117961198091507, 396.5935363769531, 358.18908278644085, 193.78872680664062], 0.5, 2), ([752.2546997070312, 391.5086669921875, 465.01080322265625, 204.017333984375], 0.5, 2), ([631.4125366210938, 318.33282470703125, 6.72271728515625, 6.756622314453125], 0.5, 9)]\n",
      "tracks   [<deep_sort_realtime.deep_sort.track.Track object at 0x7aaf582447c0>, <deep_sort_realtime.deep_sort.track.Track object at 0x7aaf582445b0>, <deep_sort_realtime.deep_sort.track.Track object at 0x7aaf58211f70>]\n",
      "(Boxes(tensor([[2.1180e-01, 3.9659e+02, 3.5840e+02, 5.9038e+02],\n",
      "        [7.5225e+02, 3.9151e+02, 1.2173e+03, 5.9553e+02],\n",
      "        [6.3141e+02, 3.1833e+02, 6.3814e+02, 3.2509e+02]], device='cuda:0')), tensor([2, 2, 9], device='cuda:0'), (720, 1280))\n",
      "Extracting scene graph for sequence 22 frame 97103 with 2 tracks\n",
      "Adding node:  Root Road Root Road\n",
      "Adding node:  ego car ego car\n",
      "Adding node:  Left Lane Left Lane\n",
      "Adding node:  Right Lane Right Lane\n",
      "Adding node:  Middle Lane Middle Lane\n",
      "Boxes:  Boxes(tensor([[2.1180e-01, 3.9659e+02, 3.5840e+02, 5.9038e+02],\n",
      "        [7.5225e+02, 3.9151e+02, 1.2173e+03, 5.9553e+02],\n",
      "        [6.3141e+02, 3.1833e+02, 6.3814e+02, 3.2509e+02]], device='cuda:0'))\n",
      "Processing bounding box:  car\n",
      "Actor:  ego_car\n",
      "Actor:  car\n",
      "Actor type:  car\n",
      "Actor actor_:  1\n",
      "Actor:  moto\n",
      "Actor:  bicycle\n",
      "Actor:  ped\n",
      "Actor:  lane\n",
      "Actor:  light\n",
      "Actor:  sign\n",
      "Actor:  road\n",
      "Actor type 1:  car\n",
      "Actor type 2:  car\n",
      "list :  ['1', '2']  list  0\n",
      "Adding node:  car_0 car_0\n",
      "Processing bounding box:  car\n",
      "Actor:  ego_car\n",
      "Actor:  car\n",
      "Actor type:  car\n",
      "Actor actor_:  1\n",
      "Actor:  moto\n",
      "Actor:  bicycle\n",
      "Actor:  ped\n",
      "Actor:  lane\n",
      "Actor:  light\n",
      "Actor:  sign\n",
      "Actor:  road\n",
      "Actor type 1:  car\n",
      "Actor type 2:  car\n",
      "list :  ['1', '2']  list  1\n",
      "Adding node:  car_1 car_1\n",
      "Processing bounding box:  traffic light\n",
      "Actor:  ego_car\n",
      "Actor:  car\n",
      "Actor:  moto\n",
      "Actor:  bicycle\n",
      "Actor:  ped\n",
      "Actor:  lane\n",
      "Actor:  light\n",
      "Actor:  sign\n",
      "Actor:  road\n",
      "Actor type 1:  \n",
      "Boxes End: \n",
      "Boxes: Boxes(tensor([[7.4217e+02, 3.9525e+02, 1.2491e+03, 6.1161e+02],\n",
      "        [8.1891e-01, 4.0133e+02, 3.3450e+02, 6.1403e+02]], device='cuda:0')), Labels: tensor([2, 2], device='cuda:0'), Image Size: (720, 1280)\n",
      "Detections: [([742.1661987304688, 395.25445556640625, 506.90924072265625, 216.35919189453125], 0.5, 2), ([0.8189058899879456, 401.3304443359375, 333.68472570180893, 212.697998046875], 0.5, 2)]\n",
      "tracks   [<deep_sort_realtime.deep_sort.track.Track object at 0x7aaf582447c0>, <deep_sort_realtime.deep_sort.track.Track object at 0x7aaf582445b0>]\n",
      "(Boxes(tensor([[7.4217e+02, 3.9525e+02, 1.2491e+03, 6.1161e+02],\n",
      "        [8.1891e-01, 4.0133e+02, 3.3450e+02, 6.1403e+02]], device='cuda:0')), tensor([2, 2], device='cuda:0'), (720, 1280))\n",
      "Extracting scene graph for sequence 22 frame 97104 with 2 tracks\n",
      "Adding node:  Root Road Root Road\n",
      "Adding node:  ego car ego car\n",
      "Adding node:  Left Lane Left Lane\n",
      "Adding node:  Right Lane Right Lane\n",
      "Adding node:  Middle Lane Middle Lane\n",
      "Boxes:  Boxes(tensor([[7.4217e+02, 3.9525e+02, 1.2491e+03, 6.1161e+02],\n",
      "        [8.1891e-01, 4.0133e+02, 3.3450e+02, 6.1403e+02]], device='cuda:0'))\n",
      "Processing bounding box:  car\n",
      "Actor:  ego_car\n",
      "Actor:  car\n",
      "Actor type:  car\n",
      "Actor actor_:  1\n",
      "Actor:  moto\n",
      "Actor:  bicycle\n",
      "Actor:  ped\n",
      "Actor:  lane\n",
      "Actor:  light\n",
      "Actor:  sign\n",
      "Actor:  road\n",
      "Actor type 1:  car\n",
      "Actor type 2:  car\n",
      "list :  ['1', '2']  list  0\n",
      "Adding node:  car_0 car_0\n",
      "Processing bounding box:  car\n",
      "Actor:  ego_car\n",
      "Actor:  car\n",
      "Actor type:  car\n",
      "Actor actor_:  1\n",
      "Actor:  moto\n",
      "Actor:  bicycle\n",
      "Actor:  ped\n",
      "Actor:  lane\n",
      "Actor:  light\n",
      "Actor:  sign\n",
      "Actor:  road\n",
      "Actor type 1:  car\n",
      "Actor type 2:  car\n",
      "list :  ['1', '2']  list  1\n",
      "Adding node:  car_1 car_1\n",
      "Boxes End: \n",
      "Boxes: Boxes(tensor([[   2.7417,  406.7518,  308.2495,  636.4703],\n",
      "        [ 742.0257,  397.4047, 1268.8766,  638.0837]], device='cuda:0')), Labels: tensor([2, 2], device='cuda:0'), Image Size: (720, 1280)\n",
      "Detections: [([2.7417449951171875, 406.75177001953125, 305.5077362060547, 229.71856689453125], 0.5, 2), ([742.0256958007812, 397.4046630859375, 526.8508911132812, 240.6790771484375], 0.5, 2)]\n",
      "tracks   [<deep_sort_realtime.deep_sort.track.Track object at 0x7aaf582447c0>, <deep_sort_realtime.deep_sort.track.Track object at 0x7aaf582445b0>]\n",
      "(Boxes(tensor([[   2.7417,  406.7518,  308.2495,  636.4703],\n",
      "        [ 742.0257,  397.4047, 1268.8766,  638.0837]], device='cuda:0')), tensor([2, 2], device='cuda:0'), (720, 1280))\n",
      "Extracting scene graph for sequence 22 frame 97105 with 2 tracks\n",
      "Adding node:  Root Road Root Road\n",
      "Adding node:  ego car ego car\n",
      "Adding node:  Left Lane Left Lane\n",
      "Adding node:  Right Lane Right Lane\n",
      "Adding node:  Middle Lane Middle Lane\n",
      "Boxes:  Boxes(tensor([[   2.7417,  406.7518,  308.2495,  636.4703],\n",
      "        [ 742.0257,  397.4047, 1268.8766,  638.0837]], device='cuda:0'))\n",
      "Processing bounding box:  car\n",
      "Actor:  ego_car\n",
      "Actor:  car\n",
      "Actor type:  car\n",
      "Actor actor_:  1\n",
      "Actor:  moto\n",
      "Actor:  bicycle\n",
      "Actor:  ped\n",
      "Actor:  lane\n",
      "Actor:  light\n",
      "Actor:  sign\n",
      "Actor:  road\n",
      "Actor type 1:  car\n",
      "Actor type 2:  car\n",
      "list :  ['1', '2']  list  0\n",
      "Adding node:  car_0 car_0\n",
      "Processing bounding box:  car\n",
      "Actor:  ego_car\n",
      "Actor:  car\n",
      "Actor type:  car\n",
      "Actor actor_:  1\n",
      "Actor:  moto\n",
      "Actor:  bicycle\n",
      "Actor:  ped\n",
      "Actor:  lane\n",
      "Actor:  light\n",
      "Actor:  sign\n",
      "Actor:  road\n",
      "Actor type 1:  car\n",
      "Actor type 2:  car\n",
      "list :  ['1', '2']  list  1\n",
      "Adding node:  car_1 car_1\n",
      "Boxes End: \n",
      "Boxes: Boxes(tensor([[ 743.3636,  401.6718, 1276.7102,  676.0959],\n",
      "        [   3.7299,  407.0073,  275.1943,  666.9420]], device='cuda:0')), Labels: tensor([2, 2], device='cuda:0'), Image Size: (720, 1280)\n",
      "Detections: [([743.3635864257812, 401.6717529296875, 533.3466186523438, 274.4241943359375], 0.5, 2), ([3.729853391647339, 407.0072937011719, 271.46448254585266, 259.9346618652344], 0.5, 2)]\n",
      "tracks   [<deep_sort_realtime.deep_sort.track.Track object at 0x7aaf582447c0>, <deep_sort_realtime.deep_sort.track.Track object at 0x7aaf582445b0>]\n",
      "(Boxes(tensor([[ 743.3636,  401.6718, 1276.7102,  676.0959],\n",
      "        [   3.7299,  407.0073,  275.1943,  666.9420]], device='cuda:0')), tensor([2, 2], device='cuda:0'), (720, 1280))\n",
      "Extracting scene graph for sequence 22 frame 97106 with 2 tracks\n",
      "Adding node:  Root Road Root Road\n",
      "Adding node:  ego car ego car\n",
      "Adding node:  Left Lane Left Lane\n",
      "Adding node:  Right Lane Right Lane\n",
      "Adding node:  Middle Lane Middle Lane\n",
      "Boxes:  Boxes(tensor([[ 743.3636,  401.6718, 1276.7102,  676.0959],\n",
      "        [   3.7299,  407.0073,  275.1943,  666.9420]], device='cuda:0'))\n",
      "Processing bounding box:  car\n",
      "Actor:  ego_car\n",
      "Actor:  car\n",
      "Actor type:  car\n",
      "Actor actor_:  1\n",
      "Actor:  moto\n",
      "Actor:  bicycle\n",
      "Actor:  ped\n",
      "Actor:  lane\n",
      "Actor:  light\n",
      "Actor:  sign\n",
      "Actor:  road\n",
      "Actor type 1:  car\n",
      "Actor type 2:  car\n",
      "list :  ['1', '2']  list  0\n",
      "Adding node:  car_0 car_0\n",
      "Processing bounding box:  car\n",
      "Actor:  ego_car\n",
      "Actor:  car\n",
      "Actor type:  car\n",
      "Actor actor_:  1\n",
      "Actor:  moto\n",
      "Actor:  bicycle\n",
      "Actor:  ped\n",
      "Actor:  lane\n",
      "Actor:  light\n",
      "Actor:  sign\n",
      "Actor:  road\n",
      "Actor type 1:  car\n",
      "Actor type 2:  car\n",
      "list :  ['1', '2']  list  1\n",
      "Adding node:  car_1 car_1\n",
      "Boxes End: \n",
      "Boxes: Boxes(tensor([[ 731.4823,  405.1859, 1280.0000,  698.3385],\n",
      "        [   0.0000,  417.6383,  240.5452,  645.9966],\n",
      "        [ 408.0329,  350.7813,  421.3022,  357.7898]], device='cuda:0')), Labels: tensor([2, 2, 2], device='cuda:0'), Image Size: (720, 1280)\n",
      "Detections: [([731.4822998046875, 405.1859130859375, 548.5177001953125, 293.152587890625], 0.5, 2), ([0.0, 417.6383056640625, 240.5452423095703, 228.35833740234375], 0.5, 2), ([408.0329284667969, 350.7812805175781, 13.26922607421875, 7.008514404296875], 0.5, 2)]\n",
      "tracks   [<deep_sort_realtime.deep_sort.track.Track object at 0x7aaf582447c0>, <deep_sort_realtime.deep_sort.track.Track object at 0x7aaf582445b0>, <deep_sort_realtime.deep_sort.track.Track object at 0x7aaf58222850>]\n",
      "(Boxes(tensor([[ 731.4823,  405.1859, 1280.0000,  698.3385],\n",
      "        [   0.0000,  417.6383,  240.5452,  645.9966],\n",
      "        [ 408.0329,  350.7813,  421.3022,  357.7898]], device='cuda:0')), tensor([2, 2, 2], device='cuda:0'), (720, 1280))\n",
      "Extracting scene graph for sequence 22 frame 97107 with 2 tracks\n",
      "Adding node:  Root Road Root Road\n",
      "Adding node:  ego car ego car\n",
      "Adding node:  Left Lane Left Lane\n",
      "Adding node:  Right Lane Right Lane\n",
      "Adding node:  Middle Lane Middle Lane\n",
      "Boxes:  Boxes(tensor([[ 731.4823,  405.1859, 1280.0000,  698.3385],\n",
      "        [   0.0000,  417.6383,  240.5452,  645.9966],\n",
      "        [ 408.0329,  350.7813,  421.3022,  357.7898]], device='cuda:0'))\n",
      "Processing bounding box:  car\n",
      "Actor:  ego_car\n",
      "Actor:  car\n",
      "Actor type:  car\n",
      "Actor actor_:  1\n",
      "Actor:  moto\n",
      "Actor:  bicycle\n",
      "Actor:  ped\n",
      "Actor:  lane\n",
      "Actor:  light\n",
      "Actor:  sign\n",
      "Actor:  road\n",
      "Actor type 1:  car\n",
      "Actor type 2:  car\n",
      "list :  ['1', '2']  list  0\n",
      "Adding node:  car_0 car_0\n",
      "Processing bounding box:  car\n",
      "Actor:  ego_car\n",
      "Actor:  car\n",
      "Actor type:  car\n",
      "Actor actor_:  1\n",
      "Actor:  moto\n",
      "Actor:  bicycle\n",
      "Actor:  ped\n",
      "Actor:  lane\n",
      "Actor:  light\n",
      "Actor:  sign\n",
      "Actor:  road\n",
      "Actor type 1:  car\n",
      "Actor type 2:  car\n",
      "list :  ['1', '2']  list  1\n",
      "Adding node:  car_1 car_1\n",
      "Processing bounding box:  car\n",
      "Actor:  ego_car\n",
      "Actor:  car\n",
      "Actor type:  car\n",
      "Actor actor_:  1\n",
      "Actor:  moto\n",
      "Actor:  bicycle\n",
      "Actor:  ped\n",
      "Actor:  lane\n",
      "Actor:  light\n",
      "Actor:  sign\n",
      "Actor:  road\n",
      "Actor type 1:  car\n",
      "Actor type 2:  car\n",
      "list :  ['1', '2']  list  2\n",
      "Adding node:  car_2 car_2\n",
      "Boxes End: \n",
      "Boxes: Boxes(tensor([[ 735.5801,  405.2082, 1278.0009,  708.1573],\n",
      "        [   1.3931,  435.0303,  208.0978,  630.9042]], device='cuda:0')), Labels: tensor([2, 2], device='cuda:0'), Image Size: (720, 1280)\n",
      "Detections: [([735.5801391601562, 405.20819091796875, 542.4207153320312, 302.94915771484375], 0.5, 2), ([1.393114447593689, 435.0302734375, 206.70469439029694, 195.87396240234375], 0.5, 2)]\n",
      "tracks   [<deep_sort_realtime.deep_sort.track.Track object at 0x7aaf582447c0>, <deep_sort_realtime.deep_sort.track.Track object at 0x7aaf582445b0>]\n",
      "(Boxes(tensor([[ 735.5801,  405.2082, 1278.0009,  708.1573],\n",
      "        [   1.3931,  435.0303,  208.0978,  630.9042]], device='cuda:0')), tensor([2, 2], device='cuda:0'), (720, 1280))\n",
      "Extracting scene graph for sequence 22 frame 97108 with 2 tracks\n",
      "Adding node:  Root Road Root Road\n",
      "Adding node:  ego car ego car\n",
      "Adding node:  Left Lane Left Lane\n",
      "Adding node:  Right Lane Right Lane\n",
      "Adding node:  Middle Lane Middle Lane\n",
      "Boxes:  Boxes(tensor([[ 735.5801,  405.2082, 1278.0009,  708.1573],\n",
      "        [   1.3931,  435.0303,  208.0978,  630.9042]], device='cuda:0'))\n",
      "Processing bounding box:  car\n",
      "Actor:  ego_car\n",
      "Actor:  car\n",
      "Actor type:  car\n",
      "Actor actor_:  1\n",
      "Actor:  moto\n",
      "Actor:  bicycle\n",
      "Actor:  ped\n",
      "Actor:  lane\n",
      "Actor:  light\n",
      "Actor:  sign\n",
      "Actor:  road\n",
      "Actor type 1:  car\n",
      "Actor type 2:  car\n",
      "list :  ['1', '2']  list  0\n",
      "Adding node:  car_0 car_0\n",
      "Processing bounding box:  car\n",
      "Actor:  ego_car\n",
      "Actor:  car\n",
      "Actor type:  car\n",
      "Actor actor_:  1\n",
      "Actor:  moto\n",
      "Actor:  bicycle\n",
      "Actor:  ped\n",
      "Actor:  lane\n",
      "Actor:  light\n",
      "Actor:  sign\n",
      "Actor:  road\n",
      "Actor type 1:  car\n",
      "Actor type 2:  car\n",
      "list :  ['1', '2']  list  1\n",
      "Adding node:  car_1 car_1\n",
      "Boxes End: \n",
      "Boxes: Boxes(tensor([[ 722.3108,  406.6585, 1280.0000,  704.6204],\n",
      "        [   2.6903,  456.3436,  158.0256,  620.7003]], device='cuda:0')), Labels: tensor([2, 2], device='cuda:0'), Image Size: (720, 1280)\n",
      "Detections: [([722.310791015625, 406.65850830078125, 557.689208984375, 297.9619140625], 0.5, 2), ([2.690330743789673, 456.3436279296875, 155.33522772789001, 164.35662841796875], 0.5, 2)]\n",
      "tracks   [<deep_sort_realtime.deep_sort.track.Track object at 0x7aaf582447c0>, <deep_sort_realtime.deep_sort.track.Track object at 0x7aaf582445b0>, <deep_sort_realtime.deep_sort.track.Track object at 0x7aaf58222cd0>]\n",
      "(Boxes(tensor([[ 722.3108,  406.6585, 1280.0000,  704.6204],\n",
      "        [   2.6903,  456.3436,  158.0256,  620.7003]], device='cuda:0')), tensor([2, 2], device='cuda:0'), (720, 1280))\n",
      "Extracting scene graph for sequence 22 frame 97109 with 2 tracks\n",
      "Adding node:  Root Road Root Road\n",
      "Adding node:  ego car ego car\n",
      "Adding node:  Left Lane Left Lane\n",
      "Adding node:  Right Lane Right Lane\n",
      "Adding node:  Middle Lane Middle Lane\n",
      "Boxes:  Boxes(tensor([[ 722.3108,  406.6585, 1280.0000,  704.6204],\n",
      "        [   2.6903,  456.3436,  158.0256,  620.7003]], device='cuda:0'))\n",
      "Processing bounding box:  car\n",
      "Actor:  ego_car\n",
      "Actor:  car\n",
      "Actor type:  car\n",
      "Actor actor_:  1\n",
      "Actor:  moto\n",
      "Actor:  bicycle\n",
      "Actor:  ped\n",
      "Actor:  lane\n",
      "Actor:  light\n",
      "Actor:  sign\n",
      "Actor:  road\n",
      "Actor type 1:  car\n",
      "Actor type 2:  car\n",
      "list :  ['1', '2']  list  0\n",
      "Adding node:  car_0 car_0\n",
      "Processing bounding box:  car\n",
      "Actor:  ego_car\n",
      "Actor:  car\n",
      "Actor type:  car\n",
      "Actor actor_:  1\n",
      "Actor:  moto\n",
      "Actor:  bicycle\n",
      "Actor:  ped\n",
      "Actor:  lane\n",
      "Actor:  light\n",
      "Actor:  sign\n",
      "Actor:  road\n",
      "Actor type 1:  car\n",
      "Actor type 2:  car\n",
      "list :  ['1', '2']  list  1\n",
      "Adding node:  car_1 car_1\n",
      "Boxes End: \n",
      "Boxes: Boxes(tensor([[ 716.2877,  407.2893, 1280.0000,  711.4319],\n",
      "        [   0.0000,  475.3673,  107.5527,  615.5495],\n",
      "        [ 393.8412,  349.9908,  409.9374,  358.1968],\n",
      "        [ 409.2044,  349.1543,  425.7847,  359.5599]], device='cuda:0')), Labels: tensor([2, 2, 2, 2], device='cuda:0'), Image Size: (720, 1280)\n",
      "Detections: [([716.2877197265625, 407.2892761230469, 563.7122802734375, 304.1426696777344], 0.5, 2), ([0.0, 475.3673095703125, 107.55265045166016, 140.18218994140625], 0.5, 2), ([393.84124755859375, 349.9908447265625, 16.096160888671875, 8.2059326171875], 0.5, 2), ([409.2044372558594, 349.1543273925781, 16.58026123046875, 10.405548095703125], 0.5, 2)]\n",
      "tracks   [<deep_sort_realtime.deep_sort.track.Track object at 0x7aaf582447c0>, <deep_sort_realtime.deep_sort.track.Track object at 0x7aaf582445b0>, <deep_sort_realtime.deep_sort.track.Track object at 0x7aaf58222cd0>, <deep_sort_realtime.deep_sort.track.Track object at 0x7aaf582392e0>, <deep_sort_realtime.deep_sort.track.Track object at 0x7aaf582393d0>]\n",
      "(Boxes(tensor([[ 716.2877,  407.2893, 1280.0000,  711.4319],\n",
      "        [   0.0000,  475.3673,  107.5527,  615.5495],\n",
      "        [ 393.8412,  349.9908,  409.9374,  358.1968],\n",
      "        [ 409.2044,  349.1543,  425.7847,  359.5599]], device='cuda:0')), tensor([2, 2, 2, 2], device='cuda:0'), (720, 1280))\n",
      "Extracting scene graph for sequence 22 frame 97110 with 2 tracks\n",
      "Adding node:  Root Road Root Road\n",
      "Adding node:  ego car ego car\n",
      "Adding node:  Left Lane Left Lane\n",
      "Adding node:  Right Lane Right Lane\n",
      "Adding node:  Middle Lane Middle Lane\n",
      "Boxes:  Boxes(tensor([[ 716.2877,  407.2893, 1280.0000,  711.4319],\n",
      "        [   0.0000,  475.3673,  107.5527,  615.5495],\n",
      "        [ 393.8412,  349.9908,  409.9374,  358.1968],\n",
      "        [ 409.2044,  349.1543,  425.7847,  359.5599]], device='cuda:0'))\n",
      "Processing bounding box:  car\n",
      "Actor:  ego_car\n",
      "Actor:  car\n",
      "Actor type:  car\n",
      "Actor actor_:  1\n",
      "Actor:  moto\n",
      "Actor:  bicycle\n",
      "Actor:  ped\n",
      "Actor:  lane\n",
      "Actor:  light\n",
      "Actor:  sign\n",
      "Actor:  road\n",
      "Actor type 1:  car\n",
      "Actor type 2:  car\n",
      "list :  ['1', '2']  list  0\n",
      "Adding node:  car_0 car_0\n",
      "Processing bounding box:  car\n",
      "Actor:  ego_car\n",
      "Actor:  car\n",
      "Actor type:  car\n",
      "Actor actor_:  1\n",
      "Actor:  moto\n",
      "Actor:  bicycle\n",
      "Actor:  ped\n",
      "Actor:  lane\n",
      "Actor:  light\n",
      "Actor:  sign\n",
      "Actor:  road\n",
      "Actor type 1:  car\n",
      "Actor type 2:  car\n",
      "list :  ['1', '2']  list  1\n",
      "Adding node:  car_1 car_1\n",
      "Processing bounding box:  car\n",
      "Actor:  ego_car\n",
      "Actor:  car\n",
      "Actor type:  car\n",
      "Actor actor_:  1\n",
      "Actor:  moto\n",
      "Actor:  bicycle\n",
      "Actor:  ped\n",
      "Actor:  lane\n",
      "Actor:  light\n",
      "Actor:  sign\n",
      "Actor:  road\n",
      "Actor type 1:  car\n",
      "Actor type 2:  car\n",
      "list :  ['1', '2']  list  2\n",
      "Adding node:  car_2 car_2\n",
      "Processing bounding box:  car\n",
      "Actor:  ego_car\n",
      "Actor:  car\n",
      "Actor type:  car\n",
      "Actor actor_:  1\n",
      "Actor:  moto\n",
      "Actor:  bicycle\n",
      "Actor:  ped\n",
      "Actor:  lane\n",
      "Actor:  light\n",
      "Actor:  sign\n",
      "Actor:  road\n",
      "Actor type 1:  car\n",
      "Actor type 2:  car\n",
      "list :  ['1', '2']  list  3\n",
      "Adding node:  car_3 car_3\n",
      "Boxes End: \n",
      "Boxes: Boxes(tensor([[ 725.7387,  417.0367, 1279.8459,  714.0009],\n",
      "        [ 405.6829,  347.7474,  423.0554,  359.2241]], device='cuda:0')), Labels: tensor([2, 2], device='cuda:0'), Image Size: (720, 1280)\n",
      "Detections: [([725.7387084960938, 417.0367431640625, 554.1072387695312, 296.964111328125], 0.5, 2), ([405.6828918457031, 347.74737548828125, 17.372528076171875, 11.4766845703125], 0.5, 2)]\n",
      "tracks   [<deep_sort_realtime.deep_sort.track.Track object at 0x7aaf582447c0>, <deep_sort_realtime.deep_sort.track.Track object at 0x7aaf582445b0>, <deep_sort_realtime.deep_sort.track.Track object at 0x7aaf582393d0>]\n",
      "(Boxes(tensor([[ 725.7387,  417.0367, 1279.8459,  714.0009],\n",
      "        [ 405.6829,  347.7474,  423.0554,  359.2241]], device='cuda:0')), tensor([2, 2], device='cuda:0'), (720, 1280))\n",
      "Extracting scene graph for sequence 22 frame 97111 with 2 tracks\n",
      "Adding node:  Root Road Root Road\n",
      "Adding node:  ego car ego car\n",
      "Adding node:  Left Lane Left Lane\n",
      "Adding node:  Right Lane Right Lane\n",
      "Adding node:  Middle Lane Middle Lane\n",
      "Boxes:  Boxes(tensor([[ 725.7387,  417.0367, 1279.8459,  714.0009],\n",
      "        [ 405.6829,  347.7474,  423.0554,  359.2241]], device='cuda:0'))\n",
      "Processing bounding box:  car\n",
      "Actor:  ego_car\n",
      "Actor:  car\n",
      "Actor type:  car\n",
      "Actor actor_:  1\n",
      "Actor:  moto\n",
      "Actor:  bicycle\n",
      "Actor:  ped\n",
      "Actor:  lane\n",
      "Actor:  light\n",
      "Actor:  sign\n",
      "Actor:  road\n",
      "Actor type 1:  car\n",
      "Actor type 2:  car\n",
      "list :  ['1', '2']  list  0\n",
      "Adding node:  car_0 car_0\n",
      "Processing bounding box:  car\n",
      "Actor:  ego_car\n",
      "Actor:  car\n",
      "Actor type:  car\n",
      "Actor actor_:  1\n",
      "Actor:  moto\n",
      "Actor:  bicycle\n",
      "Actor:  ped\n",
      "Actor:  lane\n",
      "Actor:  light\n",
      "Actor:  sign\n",
      "Actor:  road\n",
      "Actor type 1:  car\n",
      "Actor type 2:  car\n",
      "list :  ['1', '2']  list  1\n",
      "Adding node:  car_1 car_1\n",
      "Boxes End: \n",
      "Boxes: Boxes(tensor([[ 729.0127,  414.2382, 1280.0000,  716.5300],\n",
      "        [ 402.5367,  348.8525,  417.7836,  359.1676]], device='cuda:0')), Labels: tensor([2, 2], device='cuda:0'), Image Size: (720, 1280)\n",
      "Detections: [([729.0126953125, 414.23822021484375, 550.9873046875, 302.291748046875], 0.5, 2), ([402.5367126464844, 348.8525085449219, 15.246917724609375, 10.3150634765625], 0.5, 2)]\n",
      "tracks   [<deep_sort_realtime.deep_sort.track.Track object at 0x7aaf582447c0>, <deep_sort_realtime.deep_sort.track.Track object at 0x7aaf582445b0>, <deep_sort_realtime.deep_sort.track.Track object at 0x7aaf582393d0>]\n",
      "(Boxes(tensor([[ 729.0127,  414.2382, 1280.0000,  716.5300],\n",
      "        [ 402.5367,  348.8525,  417.7836,  359.1676]], device='cuda:0')), tensor([2, 2], device='cuda:0'), (720, 1280))\n",
      "Extracting scene graph for sequence 22 frame 97112 with 3 tracks\n",
      "Adding node:  Root Road Root Road\n",
      "Adding node:  ego car ego car\n",
      "Adding node:  Left Lane Left Lane\n",
      "Adding node:  Right Lane Right Lane\n",
      "Adding node:  Middle Lane Middle Lane\n",
      "Boxes:  Boxes(tensor([[ 729.0127,  414.2382, 1280.0000,  716.5300],\n",
      "        [ 402.5367,  348.8525,  417.7836,  359.1676]], device='cuda:0'))\n",
      "Processing bounding box:  car\n",
      "Actor:  ego_car\n",
      "Actor:  car\n",
      "Actor type:  car\n",
      "Actor actor_:  1\n",
      "Actor:  moto\n",
      "Actor:  bicycle\n",
      "Actor:  ped\n",
      "Actor:  lane\n",
      "Actor:  light\n",
      "Actor:  sign\n",
      "Actor:  road\n",
      "Actor type 1:  car\n",
      "Actor type 2:  car\n",
      "list :  ['1', '2', '8']  list  0\n",
      "Adding node:  car_0 car_0\n",
      "Processing bounding box:  car\n",
      "Actor:  ego_car\n",
      "Actor:  car\n",
      "Actor type:  car\n",
      "Actor actor_:  1\n",
      "Actor:  moto\n",
      "Actor:  bicycle\n",
      "Actor:  ped\n",
      "Actor:  lane\n",
      "Actor:  light\n",
      "Actor:  sign\n",
      "Actor:  road\n",
      "Actor type 1:  car\n",
      "Actor type 2:  car\n",
      "list :  ['1', '2', '8']  list  1\n",
      "Adding node:  car_1 car_1\n",
      "Boxes End: \n",
      "Boxes: Boxes(tensor([[ 720.2221,  421.7796, 1277.9071,  711.7578]], device='cuda:0')), Labels: tensor([2], device='cuda:0'), Image Size: (720, 1280)\n",
      "Detections: [([720.2221069335938, 421.7795715332031, 557.6849975585938, 289.9782409667969], 0.5, 2)]\n",
      "tracks   [<deep_sort_realtime.deep_sort.track.Track object at 0x7aaf582447c0>, <deep_sort_realtime.deep_sort.track.Track object at 0x7aaf582445b0>, <deep_sort_realtime.deep_sort.track.Track object at 0x7aaf582393d0>]\n",
      "(Boxes(tensor([[ 720.2221,  421.7796, 1277.9071,  711.7578]], device='cuda:0')), tensor([2], device='cuda:0'), (720, 1280))\n",
      "Extracting scene graph for sequence 22 frame 97113 with 3 tracks\n",
      "Adding node:  Root Road Root Road\n",
      "Adding node:  ego car ego car\n",
      "Adding node:  Left Lane Left Lane\n",
      "Adding node:  Right Lane Right Lane\n",
      "Adding node:  Middle Lane Middle Lane\n",
      "Boxes:  Boxes(tensor([[ 720.2221,  421.7796, 1277.9071,  711.7578]], device='cuda:0'))\n",
      "Processing bounding box:  car\n",
      "Actor:  ego_car\n",
      "Actor:  car\n",
      "Actor type:  car\n",
      "Actor actor_:  1\n",
      "Actor:  moto\n",
      "Actor:  bicycle\n",
      "Actor:  ped\n",
      "Actor:  lane\n",
      "Actor:  light\n",
      "Actor:  sign\n",
      "Actor:  road\n",
      "Actor type 1:  car\n",
      "Actor type 2:  car\n",
      "list :  ['1', '2', '8']  list  0\n",
      "Adding node:  car_0 car_0\n",
      "Boxes End: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:10<00:00, 10.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boxes: Boxes(tensor([[ 706.4570,  435.7354, 1274.6490,  714.3795]], device='cuda:0')), Labels: tensor([2], device='cuda:0'), Image Size: (720, 1280)\n",
      "Detections: [([706.45703125, 435.7353515625, 568.1920166015625, 278.64410400390625], 0.5, 2)]\n",
      "tracks   [<deep_sort_realtime.deep_sort.track.Track object at 0x7aaf582447c0>, <deep_sort_realtime.deep_sort.track.Track object at 0x7aaf582393d0>]\n",
      "(Boxes(tensor([[ 706.4570,  435.7354, 1274.6490,  714.3795]], device='cuda:0')), tensor([2], device='cuda:0'), (720, 1280))\n",
      "Extracting scene graph for sequence 22 frame 97114 with 2 tracks\n",
      "Adding node:  Root Road Root Road\n",
      "Adding node:  ego car ego car\n",
      "Adding node:  Left Lane Left Lane\n",
      "Adding node:  Right Lane Right Lane\n",
      "Adding node:  Middle Lane Middle Lane\n",
      "Boxes:  Boxes(tensor([[ 706.4570,  435.7354, 1274.6490,  714.3795]], device='cuda:0'))\n",
      "Processing bounding box:  car\n",
      "Actor:  ego_car\n",
      "Actor:  car\n",
      "Actor type:  car\n",
      "Actor actor_:  1\n",
      "Actor:  moto\n",
      "Actor:  bicycle\n",
      "Actor:  ped\n",
      "Actor:  lane\n",
      "Actor:  light\n",
      "Actor:  sign\n",
      "Actor:  road\n",
      "Actor type 1:  car\n",
      "Actor type 2:  car\n",
      "list :  ['1', '8']  list  0\n",
      "Adding node:  car_0 car_0\n",
      "Boxes End: \n",
      "Saving dataset to /home/irfan/roadscene2vec/examples/object_based_sg_extraction_output.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "scenegraph_extraction_config = configuration(r\"/home/irfan/roadscene2vec/examples/use_case_2_scenegraph_extraction_config.yaml\",from_function = True) #create scenegraph extraction config object\n",
    "\n",
    "sg_extraction_object = RealEx.RealExtractor(scenegraph_extraction_config) #creating Real Image Preprocessor using config\n",
    "sg_extraction_object.load() #preprocesses sequences by extracting frame data for each sequence\n",
    "scenegraph_dataset = sg_extraction_object.getDataSet() #returned scenegraphs from extraction\n",
    "scenegraph_dataset.save() #save ScenegraphDataset\n",
    "\n",
    "\n",
    "# extracted_scenegraphs = extract_seq(scenegraph_extraction_config) #extracted scenegraphs for each frame for the given sequence into a ScenegraphDataset \n",
    "# training_config = configuration(\"/home/irfan/roadscene2vec/examples/use_case_2_learning_config.yaml\",from_function = True) #create training config object                                                                                                               \n",
    "# trainer = Scenegraph_Trainer(training_config) #create trainer object using config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12ce3ddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences: [22]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'SceneGraphDataset' object has no attribute 'scene_graphs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m scenegraph_dataset\u001b[38;5;241m.\u001b[39mlabels\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ind, seq \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(sorted_seq): \n\u001b[0;32m----> 6\u001b[0m                 data_to_append \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msequence\u001b[39m\u001b[38;5;124m\"\u001b[39m:trainer\u001b[38;5;241m.\u001b[39mscene_graph_dataset\u001b[38;5;241m.\u001b[39mprocess_real_image_graph_sequences(\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscene_graph_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscene_graphs\u001b[49m[seq], trainer\u001b[38;5;241m.\u001b[39mfeature_list, folder_name \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mscene_graph_dataset\u001b[38;5;241m.\u001b[39mfolder_names[ind] ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m:trainer\u001b[38;5;241m.\u001b[39mscene_graph_dataset\u001b[38;5;241m.\u001b[39mlabels[seq], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfolder_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: trainer\u001b[38;5;241m.\u001b[39mscene_graph_dataset\u001b[38;5;241m.\u001b[39mfolder_names[ind]}\n\u001b[1;32m      7\u001b[0m                 \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_to_append\u001b[39m\u001b[38;5;124m\"\u001b[39m, data_to_append)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SceneGraphDataset' object has no attribute 'scene_graphs'"
     ]
    }
   ],
   "source": [
    "sorted_seq = sorted(scenegraph_dataset.labels)\n",
    "print(f\"Number of sequences: {(sorted_seq)}\")\n",
    "scenegraph_dataset.labels\n",
    "\n",
    "for ind, seq in enumerate(sorted_seq): \n",
    "                data_to_append = {\"sequence\":trainer.scene_graph_dataset.process_real_image_graph_sequences(trainer.scene_graph_dataset.scene_graphs[seq], trainer.feature_list, folder_name = trainer.scene_graph_dataset.folder_names[ind] ), \"label\":trainer.scene_graph_dataset.labels[seq], \"folder_name\": trainer.scene_graph_dataset.folder_names[ind]}\n",
    "                print(\"data_to_append\", data_to_append)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "345094b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_config = configuration(\"/home/irfan/roadscene2vec/examples/use_case_2_learning_config.yaml\",from_function = True) #create training config object                                                                                                               \n",
    "trainer = Scenegraph_Trainer(training_config) #create trainer object using config\n",
    "# trainer.split_dataset() #split ScenegraphDataset specified in learning config into training, testing data\n",
    "# trainer.build_model() #build model specified in learning config\n",
    "# trainer.learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c0a0141",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  training_config = configuration(r\"use_case_2_learning_config.yaml\",from_function = True) #create training config object                                                                                                               \n",
    "    # trainer = Scenegraph_Trainer(training_config) #create trainer object using config\n",
    "sys.modules['util'] = roadscene2vec.util\n",
    "\n",
    "\n",
    "def format_use_case_model_input(sequence, trainer):\n",
    "        if trainer.config.training_configuration[\"scenegraph_dataset_type\"] == \"carla\":\n",
    "            for seq in sequence.scene_graphs:\n",
    "                data = {\"sequence\":trainer.scene_graph_dataset.process_carla_graph_sequences(sequence.scene_graphs[seq], feature_list = trainer.feature_list, folder_name = sequence.folder_names[0]) , \"label\":None, \"folder_name\": sequence.folder_names[0]}\n",
    "        elif trainer.config.training_configuration[\"scenegraph_dataset_type\"] == \"real\": #by default the ScenegraphDataset extracted from a \"real\" image based sequence\n",
    "            for seq in sequence.scene_graphs:\n",
    "                data = {\"sequence\":trainer.scene_graph_dataset.process_real_image_graph_sequences(sequence.scene_graphs[seq], feature_list = trainer.feature_list, folder_name = sequence.folder_names[0]) , \"label\":None, \"folder_name\": sequence.folder_names[0]}\n",
    "        else:\n",
    "            raise ValueError('output():scenegraph_dataset_type unrecognized')\n",
    "        data = data['sequence']\n",
    "        graph_list = [Data(x=g['node_features'], edge_index=g['edge_index'], edge_attr=g['edge_attr']) for g in data]  \n",
    "        train_loader = DataLoader(graph_list, batch_size=len(graph_list))\n",
    "        sequence = next(iter(train_loader)).to(trainer.config.model_configuration[\"device\"])\n",
    "        \n",
    "        return (sequence.x, sequence.edge_index, sequence.edge_attr, sequence.batch)    \n",
    "\n",
    "\n",
    "def extract_seq(scenegraph_extraction_config):                                                                                          \n",
    "    sg_extraction_object = RealEx.RealExtractor(scenegraph_extraction_config) #creating Real Image Preprocessor using config\n",
    "    sg_extraction_object.load() #preprocesses sequences by extracting frame data for each sequence\n",
    "    scenegraph_dataset = sg_extraction_object.getDataSet() #returned scenegraphs from extraction\n",
    "    scenegraph_dataset.save() #save ScenegraphDataset\n",
    "    return scenegraph_dataset #return ScenegraphDataset\n",
    "    \n",
    "def risk_assess():\n",
    "    scenegraph_extraction_config = configuration(r\"/home/irfan/roadscene2vec/examples/use_case_2_learning_config.yaml\",from_function = True) #create scenegraph extraction config object\n",
    "    extracted_scenegraphs = extract_seq(scenegraph_extraction_config) #extracted scenegraphs for each frame for the given sequence into a ScenegraphDataset \n",
    "    training_config = configuration(\"/home/irfan/roadscene2vec/examples/use_case_2_learning_config.yaml\",from_function = True) #create training config object                                                                                                               \n",
    "    trainer = Scenegraph_Trainer(training_config) #create trainer object using config\n",
    "    trainer.split_dataset() #split ScenegraphDataset specified in learning config into training, testing data\n",
    "    trainer.build_model() #build model specified in learning config\n",
    "    trainer.learn()\n",
    "    model_input = format_use_case_model_input(extracted_scenegraphs, trainer) #turn extracted original sequence's extracted ScenegraphDataset into model input\n",
    "    output, _ = trainer.model.forward(*model_input) #output risk assessment for the original sequence \n",
    "    return output   \n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     print(risk_assess()) #Assess risk of a driving sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1aadfbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "from roadscene2vec.learning.model.cnn_lstm import CNN_LSTM_Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84f48e7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading dataset /home/irfan/roadscene2vec/examples/use_case_1_sg_extraction_output.pkl\n"
     ]
    }
   ],
   "source": [
    "import sys, pdb\n",
    "# from pathlib import Path\n",
    "# sys.path.append(str(Path(\"../../\")))\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from tqdm import tqdm\n",
    "from roadscene2vec.data.dataset import SceneGraphDataset\n",
    "# from torch_geometric.data import Data, DataLoader, DataListLoader \n",
    "# from  util import *\n",
    "from roadscene2vec.learning.util.metrics import get_metrics, log_wandb, log_wandb_transfer_learning \n",
    "from roadscene2vec.learning.util import *\n",
    "\n",
    "scene_graph_dataset  = SceneGraphDataset()\n",
    "scene_graph_dataset.dataset_save_path =\"/home/irfan/roadscene2vec/examples/use_case_1_sg_extraction_output.pkl\"\n",
    "scene_graph_dataset_ = scene_graph_dataset.load()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9defc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data, DataLoader, DataListLoader \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0094b62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from tqdm import tqdm\n",
    "from roadscene2vec.learning.util.trainer import Trainer\n",
    "from roadscene2vec.data.dataset import SceneGraphDataset\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader, DataListLoader\n",
    "from roadscene2vec.learning.util.metrics import get_metrics, log_wandb, log_wandb_transfer_learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e42a21c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from roadscene2vec.util.config_parser import configuration\n",
    "# import roadscene2vec.scene_graph.extraction.image_extractor as RealEx\n",
    "from roadscene2vec.learning.util.scenegraph_trainer import Scenegraph_Trainer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1a28aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import roadscene2vec\n",
    "from roadscene2vec.util.config_parser import configuration\n",
    "import roadscene2vec.scene_graph.extraction.image_extractor as RealEx\n",
    "from roadscene2vec.learning.util.scenegraph_trainer import Scenegraph_Trainer\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "training_config = configuration(r\"/home/irfan/roadscene2vec/examples/use_case_2_learning_config.yaml\",from_function = True) #create training config object                                                                                                               \n",
    "trainer = Scenegraph_Trainer(training_config) #create trainer object using config\n",
    "# trainer.build_transfer_learning_dataset() #build the dataset for transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd46c277",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SceneGraphDataset' object has no attribute 'labels'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscene_graph_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabels\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SceneGraphDataset' object has no attribute 'labels'"
     ]
    }
   ],
   "source": [
    "trainer.scene_graph_dataset.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcb724f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SceneGraphDataset' object has no attribute 'labels'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m sorted_seq \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscene_graph_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabels\u001b[49m)\n\u001b[1;32m      2\u001b[0m feature_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m9\u001b[39m):\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SceneGraphDataset' object has no attribute 'labels'"
     ]
    }
   ],
   "source": [
    "sorted_seq = sorted(trainer.scene_graph_dataset.labels)\n",
    "feature_list = set()\n",
    "for i in range(9):\n",
    "    feature_list.add(\"type_\"+str(i))\n",
    "# sorted_seq = sorted(scene_graph_dataset_.labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4520690c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading dataset /home/irfan/roadscene2vec/examples/use_case_1_sg_extraction_output.pkl\n",
      "Number of sequences: [22]\n"
     ]
    }
   ],
   "source": [
    "scene_graph_dataset  = SceneGraphDataset()\n",
    "scene_graph_dataset.dataset_save_path =\"/home/irfan/roadscene2vec/examples/use_case_1_sg_extraction_output.pkl\"\n",
    "scene_graph_dataset_ = scene_graph_dataset.load() \n",
    "sorted_seq = sorted(scene_graph_dataset_.labels)\n",
    "print(f\"Number of sequences: {(sorted_seq)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b884e935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence: 22, Folder name: 0\n",
      "Sequence: 22, Number of frames: {97095: <roadscene2vec.scene_graph.scene_graph.SceneGraph object at 0x74b05628dc10>, 97096: <roadscene2vec.scene_graph.scene_graph.SceneGraph object at 0x74aff03db070>, 97097: <roadscene2vec.scene_graph.scene_graph.SceneGraph object at 0x74aff4c3f6d0>, 97098: <roadscene2vec.scene_graph.scene_graph.SceneGraph object at 0x74aff0484f70>, 97099: <roadscene2vec.scene_graph.scene_graph.SceneGraph object at 0x74aff0489ee0>, 97100: <roadscene2vec.scene_graph.scene_graph.SceneGraph object at 0x74aff03c53a0>, 97101: <roadscene2vec.scene_graph.scene_graph.SceneGraph object at 0x74aff05a5f10>, 97102: <roadscene2vec.scene_graph.scene_graph.SceneGraph object at 0x74aff053bd30>, 97103: <roadscene2vec.scene_graph.scene_graph.SceneGraph object at 0x74aff03ffdc0>, 97104: <roadscene2vec.scene_graph.scene_graph.SceneGraph object at 0x74aff0458460>, 97105: <roadscene2vec.scene_graph.scene_graph.SceneGraph object at 0x74aff4bc7e20>, 97106: <roadscene2vec.scene_graph.scene_graph.SceneGraph object at 0x74aff0471730>, 97107: <roadscene2vec.scene_graph.scene_graph.SceneGraph object at 0x74aff058d5b0>, 97108: <roadscene2vec.scene_graph.scene_graph.SceneGraph object at 0x74aff4c71460>, 97109: <roadscene2vec.scene_graph.scene_graph.SceneGraph object at 0x74aff0389fa0>, 97110: <roadscene2vec.scene_graph.scene_graph.SceneGraph object at 0x74aff04959a0>, 97111: <roadscene2vec.scene_graph.scene_graph.SceneGraph object at 0x74aff0424e50>, 97112: <roadscene2vec.scene_graph.scene_graph.SceneGraph object at 0x74affffacf40>, 97113: <roadscene2vec.scene_graph.scene_graph.SceneGraph object at 0x74aff4c2af40>, 97114: <roadscene2vec.scene_graph.scene_graph.SceneGraph object at 0x74aff4c2ae20>}\n",
      "Sequence: 22, Number of frames: [97095, 97096, 97097, 97098, 97099, 97100, 97101, 97102, 97103, 97104, 97105, 97106, 97107, 97108, 97109, 97110, 97111, 97112, 97113, 97114]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'feature_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 19\u001b[0m\n\u001b[1;32m     14\u001b[0m sg_dict \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     16\u001b[0m node_name2idx \u001b[38;5;241m=\u001b[39m {node: idx \u001b[38;5;28;01mfor\u001b[39;00m idx,\n\u001b[1;32m     17\u001b[0m                  node \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(scenegraph\u001b[38;5;241m.\u001b[39mg\u001b[38;5;241m.\u001b[39mnodes)}\n\u001b[0;32m---> 19\u001b[0m sg_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnode_features\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m scenegraph\u001b[38;5;241m.\u001b[39mget_real_image_node_embeddings(\u001b[43mfeature_list\u001b[49m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNode features: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msg_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnode_features\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     21\u001b[0m sg_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124medge_index\u001b[39m\u001b[38;5;124m'\u001b[39m], sg_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124medge_attr\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m scenegraph\u001b[38;5;241m.\u001b[39mget_real_image_edge_embeddings(node_name2idx)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'feature_list' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "for ind, seq in enumerate(sorted_seq): \n",
    "    print(f\"Sequence: {seq}, Folder name: {ind}\")\n",
    "    scenegraphs=scene_graph_dataset_.scene_graphs[seq]\n",
    "    print(f\"Sequence: {seq}, Number of frames: {scenegraphs}\")\n",
    "    frame_numbers = sorted(list(scenegraphs.keys()))\n",
    "    print(f\"Sequence: {seq}, Number of frames: {(frame_numbers)}\") \n",
    "    \n",
    "    if frame_numbers == None:\n",
    "            frame_numbers = sorted(list(scenegraphs.keys()))\n",
    "    scenegraphs = [scenegraphs[frames] for frames in sorted(scenegraphs.keys())]\n",
    "    sequence = []\n",
    "    \n",
    "    for idx, (scenegraph, frame_number) in enumerate(zip(scenegraphs, frame_numbers)):\n",
    "            sg_dict = {}\n",
    "    \n",
    "            node_name2idx = {node: idx for idx,\n",
    "                             node in enumerate(scenegraph.g.nodes)}\n",
    "    \n",
    "            sg_dict['node_features'] = scenegraph.get_real_image_node_embeddings(feature_list)\n",
    "            print(f\"Node features: {sg_dict['node_features']}\")\n",
    "            sg_dict['edge_index'], sg_dict['edge_attr'] = scenegraph.get_real_image_edge_embeddings(node_name2idx)\n",
    "            print(f\"Edge index: {sg_dict['edge_index']}, Edge attr: {sg_dict['edge_attr']}\")\n",
    "            # sg_dict['folder_name'] = folder_name\n",
    "            sg_dict['frame_number'] = frame_number\n",
    "            sg_dict['node_order'] = node_name2idx\n",
    "            sequence.append(sg_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "22696cac",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SceneGraphDataset' object has no attribute 'keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# sorted_seq = sorted(scene_graph_dataset_.labels)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ind, seq \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(sorted_seq): \n\u001b[0;32m----> 6\u001b[0m                 data_to_append \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msequence\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[43mscene_graph_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_real_image_graph_sequences\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscene_graph_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43mfeature_list\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m:scene_graph_dataset\u001b[38;5;241m.\u001b[39mlabels[seq]}\n\u001b[1;32m      7\u001b[0m                 \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_to_append\u001b[39m\u001b[38;5;124m\"\u001b[39m, data_to_append)\n",
      "File \u001b[0;32m~/roadscene2vec/roadscene2vec/data/dataset.py:118\u001b[0m, in \u001b[0;36mSceneGraphDataset.process_real_image_graph_sequences\u001b[0;34m(self, scenegraphs, feature_list, frame_numbers, folder_name)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m    Creates trainer input using image data.\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;124;03m    The self.scenegraphs_sequence should be having same length after the subsampling. \u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;124;03m    This function will get the graph-related features (node embeddings, edge types, adjacency matrix) from scenegraphs.\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;124;03m    in tensor formats.\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m frame_numbers \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 118\u001b[0m     frame_numbers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mlist\u001b[39m(\u001b[43mscenegraphs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m()))\n\u001b[1;32m    119\u001b[0m scenegraphs \u001b[38;5;241m=\u001b[39m [scenegraphs[frames] \u001b[38;5;28;01mfor\u001b[39;00m frames \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(scenegraphs\u001b[38;5;241m.\u001b[39mkeys())]\n\u001b[1;32m    120\u001b[0m sequence \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SceneGraphDataset' object has no attribute 'keys'"
     ]
    }
   ],
   "source": [
    "feature_list = set()\n",
    "for i in range(9):\n",
    "    feature_list.add(\"type_\"+str(i))\n",
    "# sorted_seq = sorted(scene_graph_dataset_.labels)\n",
    "for ind, seq in enumerate(sorted_seq): \n",
    "                data_to_append = {\"sequence\":scene_graph_dataset.process_real_image_graph_sequences(scene_graph_dataset,feature_list ), \"label\":scene_graph_dataset.labels[seq]}\n",
    "                print(\"data_to_append\", data_to_append)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "82670204",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SceneGraphDataset' object has no attribute 'labels'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m sorted_seq \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscene_graph_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabels\u001b[49m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ind, seq \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(sorted_seq): \n\u001b[1;32m      3\u001b[0m     scenegraphs\u001b[38;5;241m=\u001b[39mscene_graph_dataset_\u001b[38;5;241m.\u001b[39mscene_graphs[seq]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SceneGraphDataset' object has no attribute 'labels'"
     ]
    }
   ],
   "source": [
    "sorted_seq = sorted(trainer.scene_graph_dataset.labels)\n",
    "for ind, seq in enumerate(sorted_seq): \n",
    "    scenegraphs=scene_graph_dataset_.scene_graphs[seq]\n",
    "    frame_numbers = sorted(list(scenegraphs.keys()))\n",
    "    print(f\"Sequence: {seq}, Number of frames: {(frame_numbers)}\") \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235c04a0",
   "metadata": {},
   "outputs": [],
   "source": [
    " self.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f718cb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[22]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9739d74d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading dataset /home/irfan/roadscene2vec/examples/use_case_1_sg_extraction_output.pkl\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "With n_samples=1, test_size=0.3 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#split ScenegraphDataset specified in learning config into training, testing data\u001b[39;00m\n\u001b[1;32m      2\u001b[0m trainer\u001b[38;5;241m.\u001b[39mbuild_model() \u001b[38;5;66;03m#build model specified in learning config\u001b[39;00m\n\u001b[1;32m      3\u001b[0m trainer\u001b[38;5;241m.\u001b[39mlearn()\n",
      "File \u001b[0;32m~/roadscene2vec/roadscene2vec/learning/util/scenegraph_trainer.py:27\u001b[0m, in \u001b[0;36mScenegraph_Trainer.split_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msplit_dataset\u001b[39m(\u001b[38;5;28mself\u001b[39m): \u001b[38;5;66;03m#this is init_dataset from multimodal\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mtraining_configuration[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtask_type\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msequence_classification\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgraph_classification\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcollision_prediction\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtesting_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_scenegraph_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal_train_labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate([np\u001b[38;5;241m.\u001b[39mfull(\u001b[38;5;28mlen\u001b[39m(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msequence\u001b[39m\u001b[38;5;124m'\u001b[39m]), data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_data]) \u001b[38;5;66;03m# used to compute frame-level class weighting\u001b[39;00m\n\u001b[1;32m     29\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal_test_labels  \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate([np\u001b[38;5;241m.\u001b[39mfull(\u001b[38;5;28mlen\u001b[39m(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msequence\u001b[39m\u001b[38;5;124m'\u001b[39m]), data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtesting_data])\n",
      "File \u001b[0;32m~/roadscene2vec/roadscene2vec/learning/util/scenegraph_trainer.py:180\u001b[0m, in \u001b[0;36mScenegraph_Trainer.build_scenegraph_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    179\u001b[0m     modified_class_0, modified_y_0 \u001b[38;5;241m=\u001b[39m class_0, y_0\n\u001b[0;32m--> 180\u001b[0m train, test, train_y, test_y \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodified_class_0\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mclass_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodified_y_0\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43my_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_configuration\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msplit_ratio\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstratify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodified_y_0\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43my_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mlocation_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransfer_path\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_transfer_learning_dataset()\n",
      "File \u001b[0;32m~/miniconda3/envs/test_env/lib/python3.9/site-packages/sklearn/model_selection/_split.py:2175\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2172\u001b[0m arrays \u001b[38;5;241m=\u001b[39m indexable(\u001b[38;5;241m*\u001b[39marrays)\n\u001b[1;32m   2174\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m _num_samples(arrays[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m-> 2175\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m \u001b[43m_validate_shuffle_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2176\u001b[0m \u001b[43m                                          \u001b[49m\u001b[43mdefault_test_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.25\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2178\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shuffle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m   2179\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stratify \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/test_env/lib/python3.9/site-packages/sklearn/model_selection/_split.py:1857\u001b[0m, in \u001b[0;36m_validate_shuffle_split\u001b[0;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[1;32m   1854\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(n_train), \u001b[38;5;28mint\u001b[39m(n_test)\n\u001b[1;32m   1856\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_train \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1857\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1858\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWith n_samples=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, test_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m and train_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, the \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1859\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresulting train set will be empty. Adjust any of the \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1860\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maforementioned parameters.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n_samples, test_size,\n\u001b[1;32m   1861\u001b[0m                                             train_size)\n\u001b[1;32m   1862\u001b[0m     )\n\u001b[1;32m   1864\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m n_train, n_test\n",
      "\u001b[0;31mValueError\u001b[0m: With n_samples=1, test_size=0.3 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
     ]
    }
   ],
   "source": [
    "trainer.split_dataset() #split ScenegraphDataset specified in learning config into training, testing data\n",
    "trainer.build_model() #build model specified in learning config\n",
    "trainer.learn()\n",
    "    # model_input = format_use_case_model_input(extracted_scenegraphs, trainer) #turn extracted original sequence's extracted ScenegraphDataset into model input\n",
    "    # output, _ = trainer.model.forward(*model_input) #output risk assessment for the original sequen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fd61a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence: 22, Number of frames: [97095, 97096, 97097, 97098, 97099, 97100, 97101, 97102, 97103, 97104, 97105, 97106, 97107, 97108, 97109, 97110, 97111, 97112, 97113, 97114]\n"
     ]
    }
   ],
   "source": [
    "sorted_seq = sorted(scene_graph_dataset_.labels)\n",
    "\n",
    "for ind, seq in enumerate(sorted_seq): \n",
    "    scenegraphs=scene_graph_dataset_.scene_graphs[seq]\n",
    "    frame_numbers = sorted(list(scenegraphs.keys()))\n",
    "    print(f\"Sequence: {seq}, Number of frames: {(frame_numbers)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "42f1826f",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 14)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m<tokenize>:14\u001b[0;36m\u001b[0m\n\u001b[0;31m    return sequence\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "for idx, scenegraph in enumerate(scenegraphs):\n",
    "            sg_dict = {}\n",
    "    \n",
    "            node_name2idx = {node: idx for idx,\n",
    "                             node in enumerate(scenegraph.g.nodes)}\n",
    "    \n",
    "            sg_dict['node_features'] = scenegraph.get_real_image_node_embeddings(feature_list)\n",
    "            sg_dict['edge_index'], sg_dict['edge_attr'] = scenegraph.get_real_image_edge_embeddings(node_name2idx)\n",
    "            sg_dict['folder_name'] = folder_name\n",
    "            sg_dict['frame_number'] = frame_number\n",
    "            sg_dict['node_order'] = node_name2idx\n",
    "            sequence.append(sg_dict)\n",
    "    \n",
    "        return sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c66c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 22 1.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for ind, seq in enumerate(sorted_seq): \n",
    "    print(ind, seq,scene_graph_dataset_.labels[seq] )\n",
    "#     data_to_append = {\"sequence\":scene_graph_dataset_.process_real_image_graph_sequences(scene_graph_dataset_.scene_graphs[seq], self.feature_list, folder_name = self.scene_graph_dataset.folder_names[ind] ), \"label\":self.scene_graph_dataset.labels[seq], \"folder_name\": self.scene_graph_dataset.folder_names[ind]}\n",
    "#                 if self.scene_graph_dataset.labels[seq] == 0:\n",
    "# #                     class_0.append(scene_graph_dataset.scene_graphs[seq]) #right now we are appending whole dictionary that contains data for all frame sg, shld we instead append each frame's sg separately\n",
    "#                     class_0.append(data_to_append)  #maybe individually for graph based and all frames together in one for seq based?\n",
    "#                                                                         #maybe instead create a massive dict with the form {seq:scene_graph_dataset.scene_graphs[seq], scene_graph_dataset.labels[seq]...}\n",
    "#                 elif self.scene_graph_dataset.labels[seq] == 1:\n",
    "#                     class_1.append(data_to_append)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88e88d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_0 = []\n",
    "        class_1 = []\n",
    "        sorted_seq = sorted(self.scene_graph_dataset.labels)\n",
    "        if self.config.training_configuration[\"scenegraph_dataset_type\"] == \"carla\":\n",
    "            for ind, seq in enumerate(sorted_seq): #for each seq in labels\n",
    "                data_to_append = {\"sequence\":self.scene_graph_dataset.process_carla_graph_sequences(self.scene_graph_dataset.scene_graphs[seq], self.feature_list, folder_name = self.scene_graph_dataset.folder_names[ind] ), \"label\":self.scene_graph_dataset.labels[seq], \"folder_name\": self.scene_graph_dataset.folder_names[ind]}\n",
    "                if self.scene_graph_dataset.labels[seq] == 0:\n",
    "#                     class_0.append(scene_graph_dataset.scene_graphs[seq]) #right now we are appending whole dictionary that contains data for all frame sg, shld we instead append each frame's sg separately\n",
    "                    class_0.append(data_to_append)  #maybe individually for graph based and all frames together in one for seq based?\n",
    "                                                                        #maybe instead create a massive dict with the form {seq:scene_graph_dataset.scene_graphs[seq], scene_graph_dataset.labels[seq]...}\n",
    "                elif self.scene_graph_dataset.labels[seq] == 1:\n",
    "                    class_1.append(data_to_append)\n",
    "                    \n",
    "        elif self.config.training_configuration[\"scenegraph_dataset_type\"] == \"real\":\n",
    "            for ind, seq in enumerate(sorted_seq): \n",
    "                data_to_append = {\"sequence\":self.scene_graph_dataset.process_real_image_graph_sequences(self.scene_graph_dataset.scene_graphs[seq], self.feature_list, folder_name = self.scene_graph_dataset.folder_names[ind] ), \"label\":self.scene_graph_dataset.labels[seq], \"folder_name\": self.scene_graph_dataset.folder_names[ind]}\n",
    "                if self.scene_graph_dataset.labels[seq] == 0:\n",
    "#                     class_0.append(scene_graph_dataset.scene_graphs[seq]) #right now we are appending whole dictionary that contains data for all frame sg, shld we instead append each frame's sg separately\n",
    "                    class_0.append(data_to_append)  #maybe individually for graph based and all frames together in one for seq based?\n",
    "                                                                        #maybe instead create a massive dict with the form {seq:scene_graph_dataset.scene_graphs[seq], scene_graph_dataset.labels[seq]...}\n",
    "                elif self.scene_graph_dataset.labels[seq] == 1:\n",
    "                    class_1.append(data_to_append)\n",
    "        elif self.config.training_configuration[\"scenegraph_dataset_type\"] != None:\n",
    "            raise ValueError('scenegraph_dataset_type not recognized') \n",
    "        \n",
    "            \n",
    "        y_0 = [0]*len(class_0)  \n",
    "        y_1 = [1]*len(class_1)\n",
    "\n",
    "        min_number = min(len(class_0), len(class_1))\n",
    "        \n",
    "        downsample = self.config.training_configuration[\"downsample\"]\n",
    "        \n",
    "        if downsample:\n",
    "            modified_class_0, modified_y_0 = resample(class_0, y_0, n_samples=min_number)\n",
    "        else:\n",
    "            modified_class_0, modified_y_0 = class_0, y_0\n",
    "        train, test, train_y, test_y = train_test_split(modified_class_0+class_1, modified_y_0+y_1, test_size=self.config.training_configuration[\"split_ratio\"], shuffle=True, stratify=modified_y_0+y_1, random_state=self.config.seed)\n",
    "        if self.config.location_data[\"transfer_path\"] != None:\n",
    "            self.build_transfer_learning_dataset()\n",
    "        #dont do kfold here instead it is done when learn() is called\n",
    "        return train, test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b59f1fc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<roadscene2vec.data.dataset.SceneGraphDataset at 0x78338ef7fa90>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scene_graph_dataset_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "02ca0c4d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# /home/irfan/roadscene2vec/roadscene2vec/data/dataset.py\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdataset\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SceneGraphDataset\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# from torch_geometric.data import Data, DataLoader, DataListLoader\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# from roadscene2vec.learning.util.metrics import get_metrics, log_wandb, log_wandb_transfer_learning \u001b[39;00m\n\u001b[1;32m     15\u001b[0m scene_graph_dataset  \u001b[38;5;241m=\u001b[39m SceneGraphDataset()\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dataset'"
     ]
    }
   ],
   "source": [
    "import sys, pdb\n",
    "# from pathlib import Pa\n",
    "# sys.path.append(str(Path(\"../../\")))\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from tqdm import tqdm\n",
    "# /home/irfan/roadscene2vec/roadscene2vec/data/dataset.py\n",
    "from dataset import SceneGraphDataset\n",
    "# from torch_geometric.data import Data, DataLoader, DataListLoader\n",
    "# from roadscene2vec.learning.util.metrics import get_metrics, log_wandb, log_wandb_transfer_learning \n",
    "\n",
    "scene_graph_dataset  = SceneGraphDataset()\n",
    "scene_graph_dataset.dataset_save_path =\"/home/irfan/Downloads/use_case_data-20250504T102722Z-001/use_case_data/271_carla_scenegraphs.pkl\"\n",
    "scene_graph_dataset_ = scene_graph_dataset.load()  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
